{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_to_batch(tensor, batch_size):\n",
    "    return tensor.unsqueeze(0).repeat(batch_size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0876,  0.0105, -0.0041,  0.0459,  0.1077, -0.0681,  0.0394, -0.0490,\n",
       "         -0.0374,  0.0253,  0.0263,  0.0250, -0.0725, -0.0289, -0.0417,  0.0821,\n",
       "          0.0226,  0.0354,  0.0285,  0.0574, -0.0257]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the agent network with CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the agent network with CNN and agent ID input\n",
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_dim, n_agents):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        # observation_shape is (H, W, C)\n",
    "        self.conv1 = nn.Conv2d(observation_shape[2], 16, kernel_size=3, stride=1, padding=1) # (16, H, W)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # (32, H, W)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1) # (32, H, W)\n",
    "\n",
    "        # Calculate the flattened size after convolutions\n",
    "        flat_size = 32 * observation_shape[0] * observation_shape[1] # 32 * H * W\n",
    "\n",
    "        # Add a linear layer to process the agent ID\n",
    "        self.fc_agent_id = nn.Linear(n_agents, 32) \n",
    "\n",
    "        self.fc1 = nn.Linear(flat_size + 32, 128)  # Concatenate conv output with agent ID embedding\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, obs, agent_id):\n",
    "        # Add a batch dimension\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = obs.unsqueeze(0) \n",
    "        if len(agent_id.shape) == 1:\n",
    "            agent_id = agent_id.unsqueeze(0)\n",
    "            \n",
    "        x = obs.permute(0, 3, 1, 2)  # Convert to (B, C, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.flatten(start_dim=1)  # Flatten all dimensions except batch\n",
    "\n",
    "        # Process agent ID\n",
    "        agent_id_embedding = F.relu(self.fc_agent_id(agent_id))\n",
    "\n",
    "        # Concatenate the flattened convolutional output with the agent ID embedding\n",
    "        x = torch.cat((x, agent_id_embedding), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values\n",
    "\n",
    "n_agents = 5  # Example: 5 agents\n",
    "agent_id = 2   # Example: Agent with ID 2 (0-indexed)\n",
    "agent_id_one_hot = F.one_hot(torch.tensor(agent_id), num_classes=n_agents).float()\n",
    "\n",
    "obs = torch.randn(13, 13, 5)\n",
    "test = AgentNetwork(obs.shape, 21, n_agents)\n",
    "test(obs, agent_id_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the HyperNetwork with CNN for dynamic weight generation\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # CNN layers\n",
    "        # Input shape (H, W, C)\n",
    "        self.conv1 = nn.Conv2d(input_shape[2], 32, kernel_size=3, padding=1) #(B, 32, H, W)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1) # (B, 16, H, W)\n",
    "        \n",
    "        # FC layers\n",
    "        flat_size = 16 * input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(flat_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Add batch dimension if not present\n",
    "        if len(state.shape) == 3:\n",
    "            state = state.unsqueeze(0)  # (1, H, W, C)\n",
    "        \n",
    "        # Convert (B, H, W, C) to (B, C, H, W) for CNN\n",
    "        x = state.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Apply convolutions\n",
    "        x = F.relu(self.conv1(x))  # (B, 32, H, W)\n",
    "        x = F.relu(self.conv2(x))  # (B, 16, H, W)\n",
    "        \n",
    "        # Flatten all dimensions except batch\n",
    "        x = x.flatten(start_dim=1)  # (B, 16*H*W)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # (B, hidden_dim)\n",
    "        weights = self.fc2(x)  # (B, output_dim)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "\n",
    "state = torch.randn(45, 45, 5)\n",
    "test_hyper = HyperNetwork(state.shape, 1, 64)\n",
    "state_batch = torch.randn(5, 45, 45, 5)\n",
    "test_hyper(state_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747],\n",
       "        [-1.3147],\n",
       "        [ 0.4179],\n",
       "        [-0.7584],\n",
       "        [-0.4372]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Mixing Network\n",
    "class MixingNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, num_agents, mixing_dim):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.mixing_dim = mixing_dim\n",
    "\n",
    "        # Hypernetworks for weights and biases\n",
    "        self.hyper_w1 = HyperNetwork(state_dim, num_agents * mixing_dim, 64)\n",
    "        self.hyper_b1 = HyperNetwork(state_dim, mixing_dim, 64)\n",
    "        self.hyper_w2 = HyperNetwork(state_dim, mixing_dim, 64)\n",
    "        self.hyper_b2 = HyperNetwork(state_dim, 1, 64)\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "        # Add batch dimension if not present\n",
    "        if len(agent_qs.shape) == 2:\n",
    "            agent_qs = agent_qs.unsqueeze(0)  # (1, num_agents, action_dim)\n",
    "        else:\n",
    "            batch_size = agent_qs.size(0)\n",
    "        if len(states.shape) == 3:\n",
    "            states = states.unsqueeze(0)  # (1, H, W, C)\n",
    "\n",
    "\n",
    "        # Compute the max value across the entire batch\n",
    "        qs_max_idx = agent_qs.argmax(dim=-1)  # Argmax of action on each agent \n",
    "        agent_qs = agent_qs.gather(1, qs_max_idx.unsqueeze(-1))  # Caculate max value on each agent\n",
    "\n",
    "        agent_qs = agent_qs.view(batch_size, 1, self.num_agents)  # (batch_size, 1, num_agents)\n",
    "\n",
    "        # First layer weights and biases\n",
    "        w1 = torch.abs(self.hyper_w1(states)) \n",
    "        w1 = w1.view(batch_size, self.num_agents, self.mixing_dim)  # (batch_size, num_agents, mixing_dim)\n",
    "        b1 = self.hyper_b1(states) # (batch_size, mixing_dim)\n",
    "        b1 = b1.view(batch_size, 1, self.mixing_dim)  # (batch_size, 1, mixing_dim)\n",
    "\n",
    "\n",
    "        # Compute first layer output\n",
    "        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)  # (batch_size, 1, mixing_dim)\n",
    "        # Second layer weights and biases\n",
    "        w2 = torch.abs(self.hyper_w2(states)) \n",
    "        w2 = w2.view(batch_size, self.mixing_dim, 1)  # (batch_size, mixing_dim, 1)\n",
    "        b2 = self.hyper_b2(states)  \n",
    "        b2 = b2.view(batch_size, 1, 1)  # (batch_size, 1, 1)\n",
    "\n",
    "        # Compute final output\n",
    "        q_tot = torch.bmm(hidden, w2) + b2  # (batch_size, 1, 1)\n",
    "        # Remove unnecessary dimensions\n",
    "        q_tot = q_tot.squeeze(-1)  # (batch_size, 1)\n",
    "        \n",
    "        # If input was single sample, remove batch dimension\n",
    "        if len(agent_qs.shape) == 2:\n",
    "            q_tot = q_tot.squeeze(0)  # (1)\n",
    "            \n",
    "        return q_tot\n",
    "\n",
    "agent_qs = torch.randn(5, 81, 21)\n",
    "test_mix_net = MixingNetwork(state.shape, 81, 2)\n",
    "test_mix_net(agent_qs, state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "class QMIX(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, state_dim, num_agents, mixing_dim):\n",
    "        super(QMIX, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # Create agent networks\n",
    "        self.agent_networks = nn.ModuleList([AgentNetwork(obs_shape, action_dim) for _ in range(num_agents)])\n",
    "\n",
    "        # Mixing network\n",
    "        self.mixing_network = MixingNetwork(state_dim, num_agents, mixing_dim)\n",
    "\n",
    "    def forward(self, total_obs, states):\n",
    "        if len(total_obs.shape) == 4:\n",
    "            total_obs = total_obs.unsqueeze(0) # (B, num_agents, H, W, C)\n",
    "            \n",
    "        # Forward pass for each agent\n",
    "        agent_qs = torch.stack([agent(total_obs[:,i]) for i, agent in enumerate(self.agent_networks)], dim=1)  # [batch_size, num_agents, action_dim]\n",
    "\n",
    "        # Mix Q-values\n",
    "        q_tot = self.mixing_network(agent_qs, states)\n",
    "        return q_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4\n",
    "\n",
    "env = battle_v4.env(map_size=45,max_cycles=300, render_mode=\"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render_modes': ['human', 'rgb_array'], 'name': 'battle_v4', 'render_fps': 5, 'is_parallelizable': True}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if not termination and not truncation:\n",
    "        action = np.random.randint(0,21)\n",
    "    else:\n",
    "        action = None\n",
    "    env.step(action)\n",
    "    \n",
    "print(env.metadata)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment parameters\n",
    "NUM_AGENTS = 81\n",
    "obs_shape = env.observation_space(\"red_0\").shape  # (Height, Width, C)\n",
    "state_dim = env.state_space.shape  # Dimension of the global state\n",
    "action_dim = env.action_space(\"red_0\").n  # Number of discrete actions for each agent\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "MIXING_DIM = 32  # Dimension of the mixing network\n",
    "NUM_EPISODES = 10\n",
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.001\n",
    "MAX_REPLAY_BUFFER_SIZE = 5000\n",
    "\n",
    "policy_net = AgentNetwork(obs_shape, action_dim, NUM_AGENTS)\n",
    "target_net = AgentNetwork(obs_shape, action_dim, NUM_AGENTS)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def one_hot_encode(agent_id):\n",
    "    return F.one_hot(torch.tensor(agent_id), num_classes=NUM_AGENTS).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "turn = \"red\"\n",
    "initial_state = np.transpose(env.state())\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        \n",
    "        agent_team = agent.split(\"_\")[0]\n",
    "        agent_id = agent.split(\"_\")[1]\n",
    "        \n",
    "        observation, reward, termination, truncation, _ = env.last()\n",
    "        done = termination or truncation\n",
    "        \n",
    "        if done:\n",
    "            action = None \n",
    "            env.step(action)\n",
    "        else:\n",
    "            if agent_team == \"blue\":\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "# for i_episode in range(episode, NUM_EPISODES):\n",
    "#     # Reset environment for the new episode\n",
    "#     env.reset()\n",
    "#     episode_reward = 0\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     # Collect experiences and interact with the environment\n",
    "#     for agent in env.agent_iter():\n",
    "#         agent_handle = agent.split(\"_\")\n",
    "\n",
    "#         observation, reward, termination, truncation, info = env.last()\n",
    "#         done = termination or truncation\n",
    "\n",
    "#         # Perform actions for each agent\n",
    "#         if done:\n",
    "#             action = None  # Agent is dead (or finished its part)\n",
    "#             env.step(action)\n",
    "#             total_obs[agent_handle[1]] = torch.tensor(0)\n",
    "#         else:\n",
    "#             if agent_handle[0] == \"blue\": \n",
    "#                 # Select action using the current policy (based on the QMIX model)\n",
    "#                 action = policy()\n",
    "#                 env.step(action)\n",
    "#                 next_observation, reward, termination, truncation, _ = env.last()\n",
    "#                 # Store the experience in replay buffer\n",
    "#                 replay_buffer.append((observation, action, reward, done, state))\n",
    "                \n",
    "#                 # Take the action in the environment\n",
    "#                 env.step(action)\n",
    "\n",
    "#                 # Calculate the next observation and the total reward for the episode\n",
    "#                 next_observation, reward, termination, truncation, info = env.last()\n",
    "                \n",
    "#                 # Store transition\n",
    "#                 replay_buffer.append((total_obs[agent], action, reward, done, env.state()))\n",
    "#                 total_obs[agent] = next_observation\n",
    "\n",
    "#                 episode_reward += reward\n",
    "\n",
    "#                 # Training step\n",
    "#                 if len(replay_buffer) >= BATCH_SIZE:\n",
    "#                     batch = np.random.choice(len(replay_buffer), BATCH_SIZE, replace=False)\n",
    "#                     transitions = [replay_buffer[idx] for idx in batch]\n",
    "#                     obs_batch, action_batch, reward_batch, done_batch, state_batch = zip(*transitions)\n",
    "\n",
    "#                     obs_batch = torch.tensor(obs_batch, dtype=torch.float32)\n",
    "#                     action_batch = torch.tensor(action_batch, dtype=torch.long)\n",
    "#                     reward_batch = torch.tensor(reward_batch, dtype=torch.float32)\n",
    "#                     done_batch = torch.tensor(done_batch, dtype=torch.float32)\n",
    "#                     state_batch = torch.tensor(state_batch, dtype=torch.float32)\n",
    "\n",
    "#                     # Forward pass\n",
    "#                     q_values, global_q = model(obs_batch, state_batch)\n",
    "\n",
    "#                     # Compute targets\n",
    "#                     with torch.no_grad():\n",
    "#                         target_q_values, _ = model(obs_batch, state_batch)\n",
    "#                         target_values = reward_batch + (1 - done_batch) * GAMMA * target_q_values.max(dim=-1)[0]\n",
    "\n",
    "#                     # Loss calculation\n",
    "#                     predicted_q_values = q_values.gather(1, action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "#                     loss = nn.MSELoss()(predicted_q_values, target_values)\n",
    "\n",
    "#                     # Optimize model\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "#                     running_loss += loss.item()\n",
    "#                 else:\n",
    "#                     action = np.random.randint(action_dim)\n",
    "#                     env.step(action)\n",
    "\n",
    "#     # Log statistics\n",
    "#     print(f\"Episode {i_episode}/{NUM_EPISODES}, Reward: {episode_reward}, Loss: {running_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
