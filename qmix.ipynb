{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heHVTofoZZO6",
        "outputId": "7c5d78c9-781d-43fe-c303-14c793859e1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-bhtctz4k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-bhtctz4k\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Collecting pettingzoo>=1.23.1 (from magent2==0.3.3)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: magent2\n",
            "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696119 sha256=b00e2506d8cdb9508f316c8eb338d879c6c4ca9333fb64622fc2883e37644591\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0aadt2go/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\n",
            "Successfully built magent2\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo, magent2\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 magent2-0.3.3 pettingzoo-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YTcIzRSpYUzC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = 42\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-_gH0kJrYUzD"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(agent_id, num_classes=81, device=\"cpu\"):\n",
        "  # Ensure agent_id is a tensor on the correct device\n",
        "  if not isinstance(agent_id, torch.Tensor):\n",
        "      agent_id = torch.tensor(agent_id, device=device)\n",
        "  else:\n",
        "      agent_id = agent_id.to(device)\n",
        "\n",
        "  return F.one_hot(agent_id.long(), num_classes=num_classes).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOG1oVyAYUzD"
      },
      "source": [
        "# Agent Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6uzGPiSTYUzE"
      },
      "outputs": [],
      "source": [
        "# Define the agent network with CNN and agent ID input\n",
        "class AgentNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_dim, n_agents):\n",
        "        super(AgentNetwork, self).__init__()\n",
        "        # observation_shape is (H, W, C)\n",
        "        self.conv1 = nn.Conv2d(observation_shape[2], 16, kernel_size=3, stride=1, padding=1) # (16, H, W)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # (32, H, W)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1) # (32, H, W)\n",
        "\n",
        "        # Calculate the flattened size after convolutions\n",
        "        flat_size = 32 * observation_shape[0] * observation_shape[1] # 32 * H * W\n",
        "\n",
        "        # Add a linear layer to process the agent ID\n",
        "        self.fc_agent_id = nn.Linear(n_agents, 32)\n",
        "\n",
        "        self.fc1 = nn.Linear(flat_size + 32, 128)  # Concatenate conv output with agent ID embedding\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, obs, agent_id):\n",
        "        agent_id = one_hot_encode(agent_id, device=agent_id.device)\n",
        "        # Add a batch dimension\n",
        "        if len(obs.shape) == 3:\n",
        "            obs = obs.unsqueeze(0)\n",
        "        x = torch.fliplr(obs).permute(0,3,1,2) # flip left-right because blue agent observe identically with red agent\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.flatten(start_dim=1)  # Flatten all dimensions except batch\n",
        "\n",
        "        # Process agent ID\n",
        "        agent_id_embedding = F.relu(self.fc_agent_id(agent_id))\n",
        "\n",
        "        # Concatenate the flattened convolutional output with the agent ID embedding\n",
        "        x = torch.cat((x, agent_id_embedding), dim=1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        q_values = self.fc2(x)\n",
        "        return q_values\n",
        "\n",
        "# n_agents = 81  # Example: 5 agents\n",
        "# agent_id = torch.randint(0, n_agents, (2,)).to(device)  # Example: Agent with ID 2 (0-indexed)\n",
        "# print(agent_id)\n",
        "# obs = torch.randn(2, 13, 13, 5).to(device)\n",
        "# test = AgentNetwork((13,13,5), 21, n_agents).to(device)\n",
        "# test(obs, agent_id).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7E2ecoyYUzE"
      },
      "source": [
        "# Hyper Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4aGFhkCLYUzE"
      },
      "outputs": [],
      "source": [
        "# Define the HyperNetwork with CNN for dynamic weight generation\n",
        "class HyperNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, output_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # CNN layers\n",
        "        # Input shape (H, W, C)\n",
        "        self.conv1 = nn.Conv2d(input_shape[2], 32, kernel_size=3, padding=1) #(B, 32, H, W)\n",
        "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1) # (B, 16, H, W)\n",
        "\n",
        "        # FC layers\n",
        "        flat_size = 16 * input_shape[0] * input_shape[1]\n",
        "        self.fc1 = nn.Linear(flat_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Add batch dimension if not present\n",
        "        if len(state.shape) == 3:\n",
        "            state = state.unsqueeze(0)  # (1, H, W, C)\n",
        "\n",
        "        # Convert (B, H, W, C) to (B, C, H, W) for CNN\n",
        "        x = state.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Apply convolutions\n",
        "        x = F.relu(self.conv1(x))  # (B, 32, H, W)\n",
        "        x = F.relu(self.conv2(x))  # (B, 16, H, W)\n",
        "\n",
        "        # Flatten all dimensions except batch\n",
        "        x = x.flatten(start_dim=1)  # (B, 16*H*W)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = F.relu(self.fc1(x))  # (B, hidden_dim)\n",
        "        weights = self.fc2(x)  # (B, output_dim)\n",
        "\n",
        "        return weights\n",
        "\n",
        "\n",
        "# state = torch.randn(45, 45, 5)\n",
        "# test_hyper = HyperNetwork(state.shape, 1, 64)\n",
        "# state_batch = torch.randn(5, 45, 45, 5)\n",
        "# test_hyper(state_batch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_M_s48jYUzF"
      },
      "source": [
        "# Mixing Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "igBS-hEXYUzF"
      },
      "outputs": [],
      "source": [
        "# Define the Mixing Network\n",
        "class MixingNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, num_agents, mixing_dim):\n",
        "        super(MixingNetwork, self).__init__()\n",
        "        self.num_agents = num_agents\n",
        "        self.mixing_dim = mixing_dim\n",
        "\n",
        "        # Hypernetworks for weights and biases\n",
        "        self.hyper_w1 = HyperNetwork(state_dim, num_agents * mixing_dim, 64)\n",
        "        self.hyper_b1 = HyperNetwork(state_dim, mixing_dim, 64)\n",
        "        self.hyper_w2 = HyperNetwork(state_dim, mixing_dim, 64)\n",
        "        self.hyper_b2 = HyperNetwork(state_dim, 1, 64)\n",
        "\n",
        "    def forward(self, agent_qs, states):\n",
        "        # Add batch dimension if not present\n",
        "        if len(agent_qs.shape) == 1:\n",
        "            agent_qs = agent_qs.unsqueeze(0)  # (1, num_agents, action_dim)\n",
        "        if len(states.shape) == 3:\n",
        "            states = states.unsqueeze(0)  # (1, H, W, C)\n",
        "\n",
        "        batch_size = agent_qs.size(0)\n",
        "\n",
        "        agent_qs = agent_qs.view(batch_size, 1, self.num_agents)  # (batch_size, 1, num_agents)\n",
        "\n",
        "        # First layer weights and biases\n",
        "        w1 = torch.abs(self.hyper_w1(states))\n",
        "        w1 = w1.view(batch_size, self.num_agents, self.mixing_dim)  # (batch_size, num_agents, mixing_dim)\n",
        "        b1 = self.hyper_b1(states) # (batch_size, mixing_dim)\n",
        "        b1 = b1.view(batch_size, 1, self.mixing_dim)  # (batch_size, 1, mixing_dim)\n",
        "\n",
        "\n",
        "        # Compute first layer output\n",
        "        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)  # (batch_size, 1, mixing_dim)\n",
        "        # Second layer weights and biases\n",
        "        w2 = torch.abs(self.hyper_w2(states))\n",
        "        w2 = w2.view(batch_size, self.mixing_dim, 1)  # (batch_size, mixing_dim, 1)\n",
        "        b2 = self.hyper_b2(states)\n",
        "        b2 = b2.view(batch_size, 1, 1)  # (batch_size, 1, 1)\n",
        "\n",
        "        # Compute final output\n",
        "        q_tot = torch.bmm(hidden, w2) + b2  # (batch_size, 1, 1)\n",
        "        # Remove unnecessary dimensions\n",
        "        q_tot = q_tot.squeeze(-1)  # (batch_size, 1)\n",
        "\n",
        "        # If input was single sample, remove batch dimension\n",
        "        if len(agent_qs.shape) == 2:\n",
        "            q_tot = q_tot.squeeze(0)  # (1)\n",
        "\n",
        "        return q_tot\n",
        "\n",
        "# agent_qs = torch.randn(5, 81)\n",
        "# test_mix_net = MixingNetwork(state.shape, 81, 2)\n",
        "# test_mix_net(agent_qs, state_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTtWdG7hYUzF"
      },
      "source": [
        "# Prioritized Multi-Agent Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "H-KWGZ3YYUzG"
      },
      "outputs": [],
      "source": [
        "class SumTree:\n",
        "    \"\"\"\n",
        "    A SumTree data structure for storing replay priorities.\n",
        "    Based on the implementation found in the paper \"Prioritized Experience Replay\" (Schaul et al., 2015).\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.data_pointer = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data\n",
        "        self.update(tree_index, priority)\n",
        "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
        "        if self.size < self.capacity:\n",
        "            self.size += 1\n",
        "\n",
        "    def update(self, tree_index, priority):\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        parent_index = 0\n",
        "        while True:\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "            else:\n",
        "                if v <= self.tree[left_child_index]:\n",
        "                    parent_index = left_child_index\n",
        "                else:\n",
        "                    v -= self.tree[left_child_index]\n",
        "                    parent_index = right_child_index\n",
        "        data_index = leaf_index - self.capacity + 1\n",
        "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "\n",
        "    def total_priority(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "class PrioritizedMultiAgentReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_frames = beta_frames\n",
        "        self.frame = 1  # For beta calculation\n",
        "        self.beta = beta_start\n",
        "\n",
        "        self.sum_tree = SumTree(capacity)\n",
        "        self.max_priority = 1.0\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        # Temporary storage for the current turn's experience\n",
        "        self.temp_experience = defaultdict(lambda: {'obs': [], 'action': [], 'reward': [], 'next_obs': [],\n",
        "                                                    'done': []})\n",
        "        self.global_state = None\n",
        "        self.next_global_state = None\n",
        "        self.turn_ended = False\n",
        "\n",
        "    def start_of_turn(self, global_state):\n",
        "        \"\"\"\n",
        "        Signal the start of a new turn and store the initial global state.\n",
        "        \"\"\"\n",
        "        self.global_state = global_state\n",
        "        self.next_global_state = None\n",
        "        self.turn_ended = False\n",
        "        self.temp_experience.clear()  # Clear any leftover data from a previous turn\n",
        "\n",
        "    def store_experience(self, agent_id, obs, action, reward, next_obs, done):\n",
        "        \"\"\"\n",
        "        Temporarily store experiences for a single agent during a turn.\n",
        "        \"\"\"\n",
        "        if self.turn_ended:\n",
        "            raise ValueError(\"Cannot store experience after the turn has ended. Call start_of_turn() first.\")\n",
        "\n",
        "        self.temp_experience[agent_id]['obs'].append(obs)\n",
        "        self.temp_experience[agent_id]['action'].append(action)\n",
        "        self.temp_experience[agent_id]['reward'].append(reward)\n",
        "        self.temp_experience[agent_id]['next_obs'].append(next_obs)\n",
        "        self.temp_experience[agent_id]['done'].append(done)\n",
        "\n",
        "    def end_of_turn(self, next_global_state):\n",
        "        \"\"\"\n",
        "        Signal the end of the turn, store the final global state, and push the complete experience to the buffer.\n",
        "        \"\"\"\n",
        "        if self.turn_ended:\n",
        "            raise ValueError(\"Turn has already ended. Call start_of_turn() to begin a new turn.\")\n",
        "\n",
        "        self.next_global_state = next_global_state\n",
        "        self.turn_ended = True\n",
        "\n",
        "        if not self.temp_experience:\n",
        "            return\n",
        "\n",
        "        # Combine all agent experiences into a single dictionary\n",
        "        full_turn_experience = {\n",
        "            'agents': {\n",
        "                agent_id: {\n",
        "                    'obs': np.array(data['obs']),\n",
        "                    'action': np.array(data['action']),\n",
        "                    'reward': np.array(data['reward']),\n",
        "                    'next_obs': np.array(data['next_obs']),\n",
        "                    'done': np.array(data['done']),\n",
        "                }\n",
        "                for agent_id, data in self.temp_experience.items()\n",
        "            },\n",
        "            'global_state': self.global_state,\n",
        "            'next_global_state': self.next_global_state\n",
        "        }\n",
        "\n",
        "        # Add full turn experience to the buffer\n",
        "        self.sum_tree.add(self.max_priority, full_turn_experience)\n",
        "\n",
        "        # Reset temporary storage (global states are reset in start_of_turn)\n",
        "        #self.temp_experience.clear()\n",
        "\n",
        "    def update_last_reward(self, agent_id, new_reward):\n",
        "        \"\"\"\n",
        "        Update the reward of the last experience for the given agent.\n",
        "        \"\"\"\n",
        "        if agent_id not in self.temp_experience or len(self.temp_experience[agent_id]['reward']) == 0:\n",
        "            return\n",
        "\n",
        "        # Get the index of the last experience of this agent\n",
        "        last_idx = len(self.temp_experience[agent_id]['reward']) - 1\n",
        "        # Update the last reward for the agent\n",
        "        self.temp_experience[agent_id]['reward'][last_idx] = new_reward\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a batch of full-turn experiences.\n",
        "        \"\"\"\n",
        "        if self.sum_tree.size < batch_size:\n",
        "            return None, None, None\n",
        "\n",
        "        segment = self.sum_tree.total_priority() / batch_size\n",
        "        priorities = []\n",
        "        batch = []\n",
        "        indices = []\n",
        "\n",
        "        self.beta = min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)\n",
        "        self.frame += 1\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            v = random.uniform(a, b)\n",
        "            index, priority, data = self.sum_tree.get_leaf(v)\n",
        "\n",
        "            if priority == 0 or data is None:\n",
        "                continue\n",
        "\n",
        "            priorities.append(priority)\n",
        "            batch.append(data)\n",
        "            indices.append(index)\n",
        "\n",
        "        if len(batch) < batch_size:\n",
        "            return None, None, None\n",
        "\n",
        "        sampling_probabilities = np.array(priorities) / self.sum_tree.total_priority()\n",
        "        is_weights = np.power(self.sum_tree.size * sampling_probabilities, -self.beta)\n",
        "        is_weights /= is_weights.max()\n",
        "\n",
        "        return batch, indices, is_weights\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        \"\"\"\n",
        "        Update the priorities of sampled full-turn experiences.\n",
        "        \"\"\"\n",
        "        for index, td_error in zip(indices, td_errors):\n",
        "            priority = (abs(td_error) + self.epsilon) ** self.alpha\n",
        "            self.sum_tree.update(index, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "\n",
        "    def sample_experience(self, batch_size, num_agents=81, device=\"cpu\"):\n",
        "        batch, indices, is_weights = self.sample(batch_size)\n",
        "\n",
        "        # Initialize arrays with correct dimensions\n",
        "        obs = np.zeros((batch_size, num_agents, 13, 13, 5), dtype=np.float32)\n",
        "        next_obs = np.zeros((batch_size, num_agents, 13, 13, 5), dtype=np.float32)\n",
        "        action = np.zeros((batch_size, num_agents), dtype=np.int64)\n",
        "        reward = np.zeros((batch_size, num_agents), dtype=np.float32)\n",
        "        done = np.zeros((batch_size, num_agents), dtype=np.float32)\n",
        "        global_state = np.zeros((batch_size, *batch[0]['global_state'].shape), dtype=np.float32)\n",
        "        next_global_state = np.zeros((batch_size, *batch[0]['next_global_state'].shape), dtype=np.float32)\n",
        "\n",
        "        # Default values for dead agents\n",
        "        default_obs = np.zeros((13, 13, 5), dtype=np.float32)\n",
        "        default_next_obs = np.zeros((13, 13, 5), dtype=np.float32)\n",
        "        default_action = 6\n",
        "        default_reward = 0.0\n",
        "        default_done = 1.0\n",
        "\n",
        "        # Populate the arrays\n",
        "        for b_idx, turn in enumerate(batch):\n",
        "            global_state[b_idx] = turn['global_state']\n",
        "            next_global_state[b_idx] = turn['next_global_state']\n",
        "\n",
        "            for agent_id in range(num_agents):\n",
        "                if agent_id in turn['agents']:\n",
        "                    agent_data = turn['agents'][agent_id]\n",
        "                    obs[b_idx, agent_id] = agent_data['obs']  # Shape matches (13, 13, 5)\n",
        "                    next_obs[b_idx, agent_id] = agent_data['next_obs']\n",
        "                    action[b_idx, agent_id] = agent_data['action'].item()\n",
        "                    reward[b_idx, agent_id] = agent_data['reward'].item()\n",
        "                    done[b_idx, agent_id] = agent_data['done'].item()\n",
        "                else:\n",
        "                    # Assign default values for missing (dead) agents\n",
        "                    obs[b_idx, agent_id] = default_obs\n",
        "                    next_obs[b_idx, agent_id] = default_next_obs\n",
        "                    action[b_idx, agent_id] = default_action\n",
        "                    reward[b_idx, agent_id] = default_reward\n",
        "                    done[b_idx, agent_id] = default_done\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        obs_batch = torch.from_numpy(obs).float().to(device)\n",
        "        next_obs_batch = torch.from_numpy(next_obs).float().to(device)\n",
        "        action_batch = torch.from_numpy(action).long().to(device)\n",
        "        reward_batch = torch.from_numpy(reward).float().to(device)\n",
        "        done_batch = torch.from_numpy(done).float().to(device)\n",
        "        global_state_batch = torch.from_numpy(global_state).float().to(device)\n",
        "        next_global_state_batch = torch.from_numpy(next_global_state).float().to(device)\n",
        "        is_weights = torch.from_numpy(is_weights).float().to(device)\n",
        "\n",
        "        return obs_batch, action_batch, reward_batch, next_obs_batch, global_state_batch, next_global_state_batch, done_batch, is_weights\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sum_tree.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgCYJNfyYUzG"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "OpDIDbsOYUzH"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "NUM_AGENTS = 81\n",
        "MIXING_DIM = 32  # Dimension of the mixing network\n",
        "NUM_EPISODES = 10\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 0.0001\n",
        "MAX_REPLAY_BUFFER_SIZE = 5000\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 10\n",
        "TAU = 0.01\n",
        "GRADIENT_CLIPPING = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shHcqjaWYUzH"
      },
      "source": [
        "# Funtions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSIfesh-YUzH"
      },
      "source": [
        "## Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nM7__6NtYUzH"
      },
      "outputs": [],
      "source": [
        "def linear_epsilon(steps_done):\n",
        "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
        "\n",
        "def policy(observation, q_network, team, id):\n",
        "    global steps_done\n",
        "    id = torch.tensor([id]).to(device)\n",
        "    sample = random.random()\n",
        "    if sample < linear_epsilon(steps_done):\n",
        "        return env.action_space(\"red_0\").sample()\n",
        "    else:\n",
        "        observation = (\n",
        "            torch.Tensor(observation).to(device)\n",
        "        )\n",
        "        if team == \"red\":\n",
        "            observation = torch.fliplr(observation)\n",
        "        with torch.no_grad():\n",
        "            q_values = q_network(observation, id)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEQVn6aYYUzH"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wE8yyHbZYUzH"
      },
      "outputs": [],
      "source": [
        "def save_model(policy_net, path=\"models/qmix.pt\"):\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "    torch.save(policy_net.state_dict(), path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEL6QxogYUzI"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "VHoTtqaoYUzI"
      },
      "outputs": [],
      "source": [
        "from magent2.environments import battle_v4\n",
        "\n",
        "env = battle_v4.env(map_size=45,max_cycles=200, step_reward = 0.01, attack_penalty=0, attack_opponent_reward=1 ,render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "\n",
        "# Define the environment parameters\n",
        "obs_shape = env.observation_space(\"red_0\").shape  # (Height, Width, C)\n",
        "state_dim = env.state_space.shape  # Dimension of the global state\n",
        "action_dim = env.action_space(\"red_0\").n  # Number of discrete actions for each agent\n",
        "\n",
        "\n",
        "policy_net = AgentNetwork(obs_shape, action_dim, NUM_AGENTS).to(device)\n",
        "mixing_net = MixingNetwork(state_dim, NUM_AGENTS, MIXING_DIM).to(device)\n",
        "target_net = AgentNetwork(obs_shape, action_dim, NUM_AGENTS).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "red_policy_net = AgentNetwork(obs_shape, action_dim, NUM_AGENTS).to(device) # for self-play\n",
        "\n",
        "\n",
        "agent_optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True, weight_decay=WEIGHT_DECAY)\n",
        "mixing_optimizer = optim.AdamW(mixing_net.parameters(), lr=LR, amsgrad=True, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "buffer = PrioritizedMultiAgentReplayBuffer(MAX_REPLAY_BUFFER_SIZE)\n",
        "\n",
        "running_loss = 0.0\n",
        "steps_done = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G20kl60GYUzI"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "nWH915nlYUzI"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    global running_loss\n",
        "\n",
        "    # Sample a batch from the prioritized multi-agent replay buffer\n",
        "    batch, indices, is_weights = buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # If the buffer doesn't have enough samples yet, return\n",
        "    if batch is None:\n",
        "        return\n",
        "    id_batch = torch.arange(81).repeat(BATCH_SIZE, 1).to(device)\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, global_state_batch, next_global_state_batch, done_batch, is_weights = buffer.sample_experience(batch_size=BATCH_SIZE, device=device)\n",
        "    # Compute individual Q-values for current observation\n",
        "    individual_q_values = torch.stack([policy_net(obs_batch[:, i], id_batch[:, i]) for i in range(NUM_AGENTS)], dim=1)  # [BATCH_SIZE, num_agents, action_dim]\n",
        "    # Compute Q-values of action that agent has taken\n",
        "    chosen_q_values = torch.gather(individual_q_values, 2, action_batch.unsqueeze(-1)).squeeze(-1) # [BATCH_SIZE, num_agents]\n",
        "\n",
        "    # Compute individual Q-values for current observation\n",
        "    next_individual_q_values = torch.stack([target_net(next_obs_batch[:, i], id_batch[:, i]) for i in range(NUM_AGENTS)], dim=1)  # [BATCH_SIZE, num_agents, action_dim]\n",
        "    target_q_values = next_individual_q_values .max(dim=-1)[0]  # [BATCH_SIZE, num_agents]\n",
        "\n",
        "    # Compute Q_tot for the current state\n",
        "    q_tot = mixing_net(chosen_q_values, global_state_batch).squeeze(1)  # [BATCH_SIZE]\n",
        "\n",
        "    # Mask for non-terminal states\n",
        "    # non_final_mask = (1 - done_batch).float()  # [BATCH_SIZE, num_agents]\n",
        "\n",
        "    # Compute Q_tot for next state using mixing network\n",
        "    next_q_tot = mixing_net(target_q_values, next_global_state_batch).squeeze(1)  # [BATCH_SIZE]\n",
        "\n",
        "    # Compute expected Q_tot\n",
        "    reward_tot = reward_batch.sum(dim=1)  # Sum rewards across agents [BATCH_SIZE]\n",
        "    expected_q_tot = reward_tot + GAMMA * next_q_tot  # Discounted target Q_tot [BATCH_SIZE]\n",
        "\n",
        "    # Compute TD errors\n",
        "    td_errors = (expected_q_tot - q_tot).detach().cpu().numpy()\n",
        "\n",
        "    # Compute Huber loss weighted by importance-sampling weights\n",
        "    criterion = nn.SmoothL1Loss(reduction='none')\n",
        "    loss_per_sample = criterion(q_tot, expected_q_tot)\n",
        "    loss = (loss_per_sample * is_weights).mean()  # Weighted loss\n",
        "\n",
        "    # Optimize the model\n",
        "    agent_optimizer.zero_grad()\n",
        "    mixing_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping for stability\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), GRADIENT_CLIPPING)\n",
        "    torch.nn.utils.clip_grad_value_(mixing_net.parameters(), GRADIENT_CLIPPING)\n",
        "\n",
        "    # Update all optimizers\n",
        "    agent_optimizer.step()\n",
        "    mixing_optimizer.step()\n",
        "\n",
        "    # Update priorities in the replay buffer\n",
        "    buffer.update_priorities(indices, td_errors)\n",
        "\n",
        "    # Track running loss\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV70Ym-2YUzJ"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tf1GQqOPYUzJ",
        "outputId": "6a0bd4a6-b0d6-4664-ca9a-47ef70b2cbfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Episode 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 1: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = -673.90, Total Steps = 400, Average Loss = 10.7728, Epsilon = 0.9800\n",
            "\n",
            "Starting Episode 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 2: 100%|██████████| 400/400 [03:09<00:00,  2.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2: Total Reward = -661.90, Total Steps = 400, Average Loss = 1.1449, Epsilon = 0.9600\n",
            "\n",
            "Starting Episode 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 3: 100%|██████████| 400/400 [03:15<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 3: Total Reward = -632.20, Total Steps = 400, Average Loss = 0.7084, Epsilon = 0.9400\n",
            "\n",
            "Starting Episode 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 4: 100%|██████████| 400/400 [03:13<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 4: Total Reward = -643.15, Total Steps = 400, Average Loss = 0.4979, Epsilon = 0.9200\n",
            "\n",
            "Starting Episode 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 5: 100%|██████████| 400/400 [03:42<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5: Total Reward = -631.44, Total Steps = 400, Average Loss = 1.0501, Epsilon = 0.9000\n",
            "\n",
            "Starting Episode 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 6: 100%|██████████| 400/400 [03:11<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 6: Total Reward = -570.81, Total Steps = 400, Average Loss = 1.0771, Epsilon = 0.8800\n",
            "\n",
            "Starting Episode 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 7: 100%|██████████| 400/400 [03:15<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 7: Total Reward = -619.37, Total Steps = 400, Average Loss = 1.6092, Epsilon = 0.8600\n",
            "\n",
            "Starting Episode 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 8: 100%|██████████| 400/400 [03:13<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 8: Total Reward = -556.01, Total Steps = 400, Average Loss = 1.3393, Epsilon = 0.8400\n",
            "\n",
            "Starting Episode 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 9: 100%|██████████| 400/400 [03:44<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 9: Total Reward = -555.66, Total Steps = 400, Average Loss = 0.9240, Epsilon = 0.8200\n",
            "\n",
            "Starting Episode 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 10: 100%|██████████| 400/400 [03:10<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: Total Reward = -427.87, Total Steps = 400, Average Loss = 1.8110, Epsilon = 0.8000\n",
            "\n",
            "Starting Episode 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 11: 100%|██████████| 400/400 [03:17<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 11: Total Reward = -445.93, Total Steps = 400, Average Loss = 2.1280, Epsilon = 0.7800\n",
            "\n",
            "Starting Episode 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 12: 100%|██████████| 400/400 [03:16<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 12: Total Reward = -448.88, Total Steps = 400, Average Loss = 1.8592, Epsilon = 0.7600\n",
            "\n",
            "Starting Episode 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 13: 100%|██████████| 400/400 [03:38<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 13: Total Reward = -413.50, Total Steps = 400, Average Loss = 2.5072, Epsilon = 0.7400\n",
            "\n",
            "Starting Episode 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 14: 100%|██████████| 400/400 [03:13<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 14: Total Reward = -393.46, Total Steps = 400, Average Loss = 2.7165, Epsilon = 0.7200\n",
            "\n",
            "Starting Episode 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 15: 100%|██████████| 400/400 [03:18<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 15: Total Reward = -398.37, Total Steps = 400, Average Loss = 2.4371, Epsilon = 0.7000\n",
            "\n",
            "Starting Episode 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 16: 100%|██████████| 400/400 [03:22<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 16: Total Reward = -420.10, Total Steps = 400, Average Loss = 2.0405, Epsilon = 0.6800\n",
            "\n",
            "Starting Episode 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 17: 100%|██████████| 400/400 [03:46<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 17: Total Reward = -315.09, Total Steps = 400, Average Loss = 4.2130, Epsilon = 0.6600\n",
            "\n",
            "Starting Episode 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 18: 100%|██████████| 400/400 [03:23<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 18: Total Reward = -440.40, Total Steps = 400, Average Loss = 3.8423, Epsilon = 0.6400\n",
            "\n",
            "Starting Episode 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 19: 100%|██████████| 400/400 [03:22<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19: Total Reward = -390.06, Total Steps = 400, Average Loss = 2.5021, Epsilon = 0.6200\n",
            "\n",
            "Starting Episode 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 20: 100%|██████████| 400/400 [03:28<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20: Total Reward = -359.54, Total Steps = 400, Average Loss = 2.3930, Epsilon = 0.6000\n",
            "\n",
            "Starting Episode 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 21: 100%|██████████| 400/400 [03:57<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 21: Total Reward = -384.45, Total Steps = 400, Average Loss = 3.3478, Epsilon = 0.5800\n",
            "\n",
            "Starting Episode 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 22: 100%|██████████| 400/400 [03:19<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 22: Total Reward = -252.57, Total Steps = 400, Average Loss = 2.3445, Epsilon = 0.5600\n",
            "\n",
            "Starting Episode 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 23: 100%|██████████| 400/400 [03:16<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 23: Total Reward = -297.74, Total Steps = 400, Average Loss = 3.5577, Epsilon = 0.5400\n",
            "\n",
            "Starting Episode 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 24: 100%|██████████| 400/400 [03:20<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 24: Total Reward = -290.75, Total Steps = 400, Average Loss = 3.2840, Epsilon = 0.5200\n",
            "\n",
            "Starting Episode 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps in Episode 25:  96%|█████████▋| 385/400 [03:39<00:08,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-9d22789872a9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0;31m# Update of the target network's weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mtarget_net_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for episode in range(NUM_EPISODES):\n",
        "    print(f\"\\nStarting Episode {episode + 1}...\")\n",
        "    prev_team = \"red\"\n",
        "    cur_team = \"red\"\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    running_loss = 0.0\n",
        "    steps_done += 1\n",
        "    total_steps = 0  # Total steps for this episode\n",
        "\n",
        "    # Reset temporary storage at the beginning of each episode\n",
        "    buffer.temp_experience.clear()\n",
        "\n",
        "    # Use tqdm to track total steps in this episode\n",
        "    with tqdm(total=400, desc=f\"Steps in Episode {episode + 1}\", leave=True) as pbar:\n",
        "        for agent in env.agent_iter():\n",
        "            agent_team = agent.split(\"_\")[0]\n",
        "            agent_id = int(agent.split(\"_\")[1])\n",
        "\n",
        "            prev_team = cur_team\n",
        "            cur_team = agent_team\n",
        "            if cur_team != prev_team:\n",
        "                total_steps += 1\n",
        "                pbar.update(1)  # Update progress bar for each step\n",
        "                state = np.transpose(env.state(), (1, 0, 2))\n",
        "                if cur_team == \"blue\":  # Red turn changes to blue turn\n",
        "                    buffer.start_of_turn(global_state=state)\n",
        "                else:  # Blue turn changes to red turn\n",
        "                    buffer.end_of_turn(next_global_state=state)\n",
        "\n",
        "            observation, reward, termination, truncation, _ = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            if done: # This agent is dead or truncated\n",
        "                action = None\n",
        "                env.step(action)\n",
        "            elif agent_team == \"blue\":\n",
        "                episode_reward += reward\n",
        "                buffer.update_last_reward(agent_id, reward)  # Update reward for the last agent's action\n",
        "\n",
        "                action = policy(observation, policy_net, agent_team, agent_id)\n",
        "                env.step(action)\n",
        "\n",
        "                next_global_state = np.transpose(env.state(), (1, 0, 2))\n",
        "                try:\n",
        "                    next_observation = env.observe(agent)\n",
        "                except:\n",
        "                    next_observation = np.zeros_like(observation)\n",
        "                    print(\"I think code never reach this line!\")\n",
        "\n",
        "                # Store the experience\n",
        "                buffer.store_experience(agent_id, observation, action, reward, next_observation, done)\n",
        "\n",
        "                # Perform one step of the optimization (on the policy network)\n",
        "                if len(buffer) % 32 == 0:\n",
        "                    optimize_model()\n",
        "                    # Update of the target network's weights\n",
        "                    with torch.no_grad():\n",
        "                        target_net_state_dict = target_net.state_dict()\n",
        "                        policy_net_state_dict = policy_net.state_dict()\n",
        "                        for key in policy_net_state_dict:\n",
        "                            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
        "                        target_net.load_state_dict(target_net_state_dict)\n",
        "            elif agent_team == \"red\":\n",
        "                # Red team actions\n",
        "                action = policy(observation, red_policy_net, agent_team, agent_id)\n",
        "                env.step(action)\n",
        "\n",
        "        pbar.refresh()  # Ensure the progress bar reflects the final step count\n",
        "\n",
        "    # Synchronize red team's policy network with blue's every 4 episodes\n",
        "    if episode % 4 == 0:\n",
        "        red_policy_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    # Summary output for each episode\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward:.2f}, Total Steps = {total_steps}, Average Loss = {running_loss:.4f}, Epsilon = {linear_epsilon(steps_done):.4f}\")\n",
        "\n",
        "    # Save the model periodically\n",
        "    save_model(policy_net, path=f\"models/blue_qmix_{episode+1}.pt\")\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}