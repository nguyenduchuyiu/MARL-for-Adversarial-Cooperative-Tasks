{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB1Z5lTngHS5",
        "outputId": "ca91e270-d64f-4bd5-f1f2-ceab5984de8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-pisufm4l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-pisufm4l\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Collecting pettingzoo>=1.23.1 (from magent2==0.3.3)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: magent2\n",
            "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696076 sha256=deb0f703ce6bef0909674d24805738625c9350687e3899c1c3431730c5266460\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yhekp7pd/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\n",
            "Successfully built magent2\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo, magent2\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 magent2-0.3.3 pettingzoo-1.24.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zWJY1eB1zT6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, action_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        x = self.cnn(x)\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        return self.network(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fskc0dibgK9X",
        "outputId": "9ca49052-0cf6-48b7-bb5a-e08768bc80ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Start with episode: 39\n",
            "Episode 40/100\n",
            "Total Reward: -48.00\n",
            "Average Loss: 294.7369\n",
            "Epsilon: 0.2799999999999999\n",
            "----------------------------------------\n",
            "Episode 41/100\n",
            "Total Reward: -145.00\n",
            "Average Loss: 57.1847\n",
            "Epsilon: 0.262\n",
            "----------------------------------------\n",
            "Episode 42/100\n",
            "Total Reward: -54.32\n",
            "Average Loss: 104.1933\n",
            "Epsilon: 0.244\n",
            "----------------------------------------\n",
            "Episode 43/100\n",
            "Total Reward: 229.60\n",
            "Average Loss: 409.3592\n",
            "Epsilon: 0.22599999999999998\n",
            "----------------------------------------\n",
            "Episode 44/100\n",
            "Total Reward: -118.29\n",
            "Average Loss: 154.3790\n",
            "Epsilon: 0.20799999999999996\n",
            "----------------------------------------\n",
            "Episode 45/100\n",
            "Total Reward: -34.01\n",
            "Average Loss: 56.1005\n",
            "Epsilon: 0.18999999999999995\n",
            "----------------------------------------\n",
            "Episode 46/100\n",
            "Total Reward: 107.65\n",
            "Average Loss: 334.6615\n",
            "Epsilon: 0.17199999999999993\n",
            "----------------------------------------\n",
            "Episode 47/100\n",
            "Total Reward: -113.35\n",
            "Average Loss: 157.3759\n",
            "Epsilon: 0.15400000000000003\n",
            "----------------------------------------\n",
            "Episode 48/100\n",
            "Total Reward: 143.31\n",
            "Average Loss: 79.5518\n",
            "Epsilon: 0.136\n",
            "----------------------------------------\n",
            "Episode 49/100\n",
            "Total Reward: -260.55\n",
            "Average Loss: 258.4848\n",
            "Epsilon: 0.118\n",
            "----------------------------------------\n",
            "Episode 50/100\n",
            "Total Reward: -75.41\n",
            "Average Loss: 140.6301\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 51/100\n",
            "Total Reward: 74.60\n",
            "Average Loss: 116.8843\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 52/100\n",
            "Total Reward: -2.00\n",
            "Average Loss: 53.3066\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 53/100\n",
            "Total Reward: 3.10\n",
            "Average Loss: 19.0256\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 54/100\n",
            "Total Reward: -50.39\n",
            "Average Loss: 307.3179\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 55/100\n",
            "Total Reward: -47.74\n",
            "Average Loss: 204.0472\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 56/100\n",
            "Total Reward: 162.93\n",
            "Average Loss: 209.8666\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 57/100\n",
            "Total Reward: -94.19\n",
            "Average Loss: 252.5987\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 58/100\n",
            "Total Reward: -19.60\n",
            "Average Loss: 64.1626\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 59/100\n",
            "Total Reward: 21.55\n",
            "Average Loss: 132.0173\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 60/100\n",
            "Total Reward: 15.79\n",
            "Average Loss: 113.9992\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 61/100\n",
            "Total Reward: 86.99\n",
            "Average Loss: 216.4901\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n",
            "Episode 62/100\n",
            "Total Reward: 22.10\n",
            "Average Loss: 125.0394\n",
            "Epsilon: 0.1\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-86ea6281c3e8>\u001b[0m in \u001b[0;36m<cell line: 190>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mpolicy_net_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_net_state_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                     \u001b[0mtarget_net_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTAU\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_net_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from collections import deque, namedtuple\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from magent2.environments import battle_v4\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def plot_metrics(episode_rewards, episode_losses, show_result=False):\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Value')\n",
        "\n",
        "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
        "    losses_t = torch.tensor(episode_losses, dtype=torch.float)\n",
        "\n",
        "    plt.plot(rewards_t.numpy(), label='Reward')\n",
        "    plt.plot(losses_t.numpy(), label='Loss')\n",
        "\n",
        "    if len(rewards_t) >= 5:\n",
        "        rewards_means = rewards_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        rewards_means = torch.cat((torch.zeros(4), rewards_means))\n",
        "        plt.plot(rewards_means.numpy(), label='Reward (mean)')\n",
        "\n",
        "    if len(losses_t) >= 5:\n",
        "        losses_means = losses_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        losses_means = torch.cat((torch.zeros(4), losses_means))\n",
        "        plt.plot(losses_means.numpy(), label='Loss (mean)')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())\n",
        "\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "env = battle_v4.env(map_size=45, minimap_mode=False, step_reward=0.01,\n",
        "                        dead_penalty=-2, attack_penalty=-0.1, attack_opponent_reward=2,\n",
        "                        max_cycles=200, extra_features=False, render_mode=\"rgb_array\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 50\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "def linear_epsilon(steps_done):\n",
        "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
        "\n",
        "def policy(observation, q_network):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    if sample < linear_epsilon(steps_done):\n",
        "        return env.action_space(\"red_0\").sample()\n",
        "    else:\n",
        "        observation = (\n",
        "            torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            q_values = q_network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    global running_loss\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path):\n",
        "    torch.save({\n",
        "        'episode': i_episode,\n",
        "        'policy_net_state_dict': policy_net.state_dict(),\n",
        "        'target_net_state_dict': target_net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_losses': episode_losses,\n",
        "    }, path)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "observation_shape = env.observation_space(\"blue_0\").shape\n",
        "action_shape = env.action_space(\"blue_0\").n\n",
        "\n",
        "# Initialize networks\n",
        "policy_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "red_policy_net = QNetwork(observation_shape, action_shape).to(device) # for self-play\n",
        "\n",
        "target_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "# Load pretrained\n",
        "pretrained_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "pretrained_net.load_state_dict(torch.load(\"red.pt\", map_location=device, weights_only=True))\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(\"models/blue.pt\", map_location=device, weights_only=True)\n",
        "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
        "    red_policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    episode = checkpoint[\"episode\"]\n",
        "    print(f\"Start with episode: {episode}\")\n",
        "except Exception as e:\n",
        "    print(f\"No model found!\")\n",
        "    episode = 0\n",
        "\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = episode\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "running_loss = 0.0\n",
        "num_episodes = 60\n",
        "\n",
        "for i_episode in range(episode, num_episodes):\n",
        "    # Initialize the environment and get its state\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    running_loss = 0.0\n",
        "    steps_done += 1\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "\n",
        "        observation, reward, termination, truncation, info = env.last()\n",
        "        done = termination or truncation\n",
        "\n",
        "        if done:\n",
        "            action = None  # Agent is dead\n",
        "            env.step(action)\n",
        "        else:\n",
        "            agent_handle = agent.split(\"_\")[0]\n",
        "            if agent_handle == \"blue\":\n",
        "                action = policy(observation, policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "                next_observation, reward, termination, truncation, info = env.last()\n",
        "\n",
        "                # Convert to tensor before feed into replay buffer\n",
        "                observation = (torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device))\n",
        "                next_observation = (torch.Tensor(next_observation).float().permute([2, 0, 1]).unsqueeze(0).to(device))\n",
        "                action = torch.tensor([action], device=device, dtype=torch.int64).unsqueeze(0)\n",
        "                reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
        "\n",
        "                # Store the transition in memory\n",
        "                memory.push(observation, action, next_observation, reward)\n",
        "\n",
        "                # Perform one step of the optimization (on the policy network)\n",
        "                optimize_model()\n",
        "\n",
        "                # Soft update of the target network's weights\n",
        "                # θ′ ← τ θ + (1 −τ )θ′\n",
        "                target_net_state_dict = target_net.state_dict()\n",
        "                policy_net_state_dict = policy_net.state_dict()\n",
        "                for key in policy_net_state_dict:\n",
        "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "                target_net.load_state_dict(target_net_state_dict)\n",
        "                # Update episode reward\n",
        "                episode_reward += reward\n",
        "            else:\n",
        "                # red agent\n",
        "                action = policy(observation, red_policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "        # Periodically update the red agent's policy with the blue agent's learned policy\n",
        "        if i_episode % 4 == 0 and i_episode < 24:\n",
        "            # Copy all weights and biases from the blue agent's policy network to the red agent's\n",
        "            red_policy_net.load_state_dict(policy_net.state_dict())\n",
        "        elif i_episode == 24: # more complex (pretrained) opponent\n",
        "            red_policy_net.load_state_dict(pretrained_net.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "    # Add these lines at the end of each episode\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_losses.append(running_loss)\n",
        "\n",
        "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
        "    print(f'Total Reward: {episode_reward.item():.2f}')\n",
        "    print(f'Average Loss: {running_loss:.4f}')\n",
        "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
        "    print('-' * 40)\n",
        "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models/blue_{i_episode}.pt\")\n",
        "\n",
        "plot_metrics(episode_rewards, episode_losses, show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
