{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB1Z5lTngHS5",
        "outputId": "ca91e270-d64f-4bd5-f1f2-ceab5984de8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-la8woob8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-la8woob8\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "zWJY1eB1zT6y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0038, -0.0428, -0.0195, -0.1008,  0.0988, -0.0643,  0.0704,  0.0239,\n",
              "         -0.1249,  0.0939,  0.0152,  0.0481, -0.0426,  0.0039,  0.0183,  0.0473,\n",
              "         -0.0691, -0.0148,  0.0266, -0.0229,  0.1093]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, action_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "            x = x.unsqueeze(0)\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "        x = torch.fliplr(x).permute(0,3,1,2) # flip left-right because blue agent observe identically with red agent\n",
        "        x = self.cnn(x)\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        return self.network(x)\n",
        "\n",
        "test = QNetwork((13,13,5), 21)\n",
        "test_obs = torch.rand((13,13,5))\n",
        "test(test_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from magent2.environments import battle_v4\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics(episode_rewards, episode_losses, show_result=False):\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Value')\n",
        "\n",
        "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
        "    losses_t = torch.tensor(episode_losses, dtype=torch.float)\n",
        "\n",
        "    plt.plot(rewards_t.numpy(), label='Reward')\n",
        "    plt.plot(losses_t.numpy(), label='Loss')\n",
        "\n",
        "    if len(rewards_t) >= 5:\n",
        "        rewards_means = rewards_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        rewards_means = torch.cat((torch.zeros(4), rewards_means))\n",
        "        plt.plot(rewards_means.numpy(), label='Reward (mean)')\n",
        "\n",
        "    if len(losses_t) >= 5:\n",
        "        losses_means = losses_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        losses_means = torch.cat((torch.zeros(4), losses_means))\n",
        "        plt.plot(losses_means.numpy(), label='Loss (mean)')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fskc0dibgK9X",
        "outputId": "9ca49052-0cf6-48b7-bb5a-e08768bc80ac"
      },
      "outputs": [],
      "source": [
        "class MultiAgentReplayBuffer:\n",
        "    def __init__(self, capacity, observation_shape, action_shape):\n",
        "        self.capacity = capacity\n",
        "        self.observation_shape = observation_shape\n",
        "        self.action_shape = action_shape\n",
        "\n",
        "        # Use a defaultdict to automatically create deques for new agents\n",
        "        self.buffers = defaultdict(lambda: {\n",
        "            'obs': deque(maxlen=capacity),\n",
        "            'action': deque(maxlen=capacity),\n",
        "            'reward': deque(maxlen=capacity),\n",
        "            'next_obs': deque(maxlen=capacity),\n",
        "            'done': deque(maxlen=capacity),\n",
        "        })\n",
        "\n",
        "    def push(self, agent_id, obs, action, reward, next_obs, done):\n",
        "        self.buffers[agent_id]['obs'].append(obs)\n",
        "        self.buffers[agent_id]['action'].append(action)\n",
        "        self.buffers[agent_id]['reward'].append(reward)\n",
        "        self.buffers[agent_id]['next_obs'].append(next_obs)\n",
        "        self.buffers[agent_id]['done'].append(done)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        all_agent_ids = list(self.buffers.keys())\n",
        "        if not all_agent_ids:\n",
        "            return None  # No agents in the buffer\n",
        "\n",
        "        # Check if we have enough data to sample\n",
        "        total_transitions = sum(len(self.buffers[agent_id]['obs']) for agent_id in all_agent_ids)\n",
        "        if total_transitions < batch_size:\n",
        "            return None\n",
        "\n",
        "        # Collect transitions from all agents into a single list\n",
        "        all_transitions = []\n",
        "        for agent_id in all_agent_ids:\n",
        "            agent_buffer = self.buffers[agent_id]\n",
        "            for i in range(len(agent_buffer['obs'])):\n",
        "                all_transitions.append({\n",
        "                    'obs': agent_buffer['obs'][i],\n",
        "                    'action': agent_buffer['action'][i],\n",
        "                    'reward': agent_buffer['reward'][i],\n",
        "                    'next_obs': agent_buffer['next_obs'][i],\n",
        "                    'done': agent_buffer['done'][i]\n",
        "                })\n",
        "\n",
        "        # Sample indices from the combined transitions\n",
        "        indices = np.random.choice(len(all_transitions), batch_size, replace=False)\n",
        "\n",
        "        # Extract the sampled transitions\n",
        "        obs_batch = np.array([all_transitions[i]['obs'] for i in indices])\n",
        "        action_batch = np.array([all_transitions[i]['action'] for i in indices])\n",
        "        reward_batch = np.array([all_transitions[i]['reward'] for i in indices])\n",
        "        next_obs_batch = np.array([all_transitions[i]['next_obs'] for i in indices])\n",
        "        done_batch = np.array([all_transitions[i]['done'] for i in indices])\n",
        "\n",
        "        return {\n",
        "            'obs': obs_batch,\n",
        "            'action': action_batch,\n",
        "            'reward': reward_batch,\n",
        "            'next_obs': next_obs_batch,\n",
        "            'done': done_batch\n",
        "        }\n",
        "\n",
        "    def update_last_reward(self, agent_id, new_reward):\n",
        "        if agent_id not in self.buffers:\n",
        "            return\n",
        "        self.buffers[agent_id]['reward'][-1] = new_reward\n",
        "        \n",
        "    def __len__(self):\n",
        "        return sum(len(self.buffers[agent_id]['obs']) for agent_id in self.buffers)\n",
        "\n",
        "    def clear(self, agent_id=None):\n",
        "        if agent_id:\n",
        "            self.buffers[agent_id]['obs'].clear()\n",
        "            self.buffers[agent_id]['action'].clear()\n",
        "            self.buffers[agent_id]['reward'].clear()\n",
        "            self.buffers[agent_id]['next_obs'].clear()\n",
        "            self.buffers[agent_id]['done'].clear()\n",
        "        else:\n",
        "            for agent_id in self.buffers:\n",
        "                self.clear(agent_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "No model found!\n"
          ]
        }
      ],
      "source": [
        "env = battle_v4.env(map_size=45, minimap_mode=False, step_reward=0.01,\n",
        "                        dead_penalty=-2, attack_penalty=-0.1, attack_opponent_reward=2,\n",
        "                        max_cycles=300, extra_features=False, render_mode=\"human\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 50\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "observation_shape = env.observation_space(\"blue_0\").shape\n",
        "action_shape = env.action_space(\"blue_0\").n\n",
        "\n",
        "# Initialize networks\n",
        "policy_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "red_policy_net = QNetwork(observation_shape, action_shape).to(device) # for self-play\n",
        "target_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "# Load pretrained\n",
        "pretrained_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "pretrained_net.load_state_dict(torch.load(\"models/red.pt\", map_location=device, weights_only=True))\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(\"models/blue.pt\", map_location=device, weights_only=True)\n",
        "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
        "    red_policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    episode = checkpoint[\"episode\"]\n",
        "    print(f\"Start with episode: {episode}\")\n",
        "except Exception as e:\n",
        "    print(f\"No model found!\")\n",
        "    episode = 0\n",
        "\n",
        "buffer = MultiAgentReplayBuffer(10000, observation_shape, action_shape)\n",
        "steps_done = episode\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "running_loss = 0.0\n",
        "num_episodes = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path):\n",
        "    torch.save({\n",
        "        'episode': i_episode,\n",
        "        'policy_net_state_dict': policy_net.state_dict(),\n",
        "        'target_net_state_dict': target_net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_losses': episode_losses,\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Greedy Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear_epsilon(steps_done):\n",
        "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
        "\n",
        "def policy(observation, q_network):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    if sample < linear_epsilon(steps_done):\n",
        "        return env.action_space(\"red_0\").sample()\n",
        "    else:\n",
        "        observation = (\n",
        "            torch.Tensor(observation).to(device)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            q_values = q_network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize_model():  \n",
        "    global running_loss\n",
        "\n",
        "    batch = buffer.sample(BATCH_SIZE)  \n",
        "\n",
        "    # Handle cases where the buffer doesn't have enough samples yet\n",
        "    if batch is None:\n",
        "        return\n",
        "\n",
        "    # Unpack the batch\n",
        "    state_batch = torch.from_numpy(batch['obs']).float().to(device)\n",
        "    action_batch = torch.from_numpy(batch['action']).long().to(device)\n",
        "    reward_batch = torch.from_numpy(batch['reward']).float().to(device)\n",
        "    next_state_batch = torch.from_numpy(batch['next_obs']).float().to(device)\n",
        "    done_batch = torch.from_numpy(batch['done']).float().to(device)\n",
        "\n",
        "    # Reshape action_batch to (BATCH_SIZE, 1) for gather()\n",
        "    action_batch = action_batch.unsqueeze(1)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    non_final_mask = (done_batch == 0).squeeze()  # Create a mask for non-terminal states\n",
        "\n",
        "    # Only compute for non-terminal states\n",
        "    if non_final_mask.any():\n",
        "        next_state_values[non_final_mask] = target_net(next_state_batch[non_final_mask]).max(1).values.detach()\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 11.7288\n",
            "Epsilon: 0.982\n",
            "----------------------------------------\n",
            "Episode 2/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 39.8582\n",
            "Epsilon: 0.964\n",
            "----------------------------------------\n",
            "Episode 3/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 94.1510\n",
            "Epsilon: 0.946\n",
            "----------------------------------------\n",
            "Episode 4/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 231.5618\n",
            "Epsilon: 0.9279999999999999\n",
            "----------------------------------------\n",
            "Episode 5/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 224.9186\n",
            "Epsilon: 0.91\n",
            "----------------------------------------\n",
            "Episode 6/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 220.0224\n",
            "Epsilon: 0.892\n",
            "----------------------------------------\n",
            "Episode 7/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 222.9506\n",
            "Epsilon: 0.874\n",
            "----------------------------------------\n",
            "Episode 8/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 213.1218\n",
            "Epsilon: 0.856\n",
            "----------------------------------------\n",
            "Episode 9/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 194.5119\n",
            "Epsilon: 0.838\n",
            "----------------------------------------\n",
            "Episode 10/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 214.0245\n",
            "Epsilon: 0.82\n",
            "----------------------------------------\n",
            "Episode 11/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 226.8025\n",
            "Epsilon: 0.802\n",
            "----------------------------------------\n",
            "Episode 12/60\n",
            "Total Reward of previous episode: 0.00\n",
            "Average Loss: 222.4212\n",
            "Epsilon: 0.784\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[98], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Periodically update the red agent's policy with the blue agent's learned policy\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i_episode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m24\u001b[39m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Copy all weights and biases from the blue agent's policy network to the red agent's\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mred_policy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i_episode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24\u001b[39m: \u001b[38;5;66;03m# more complex (pretrained) opponent\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     red_policy_net\u001b[38;5;241m.\u001b[39mload_state_dict(pretrained_net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
            "File \u001b[0;32m~/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2564\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2558\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2562\u001b[0m         )\n\u001b[0;32m-> 2564\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
            "File \u001b[0;32m~/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2535\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2534\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2535\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:2351\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_state_dict_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   2341\u001b[0m     hook(\n\u001b[1;32m   2342\u001b[0m         state_dict,\n\u001b[1;32m   2343\u001b[0m         prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2348\u001b[0m         error_msgs,\n\u001b[1;32m   2349\u001b[0m     )\n\u001b[0;32m-> 2351\u001b[0m persistent_buffers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2352\u001b[0m     k: v\n\u001b[1;32m   2353\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set\n\u001b[1;32m   2355\u001b[0m }\n\u001b[1;32m   2356\u001b[0m local_name_params \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(), persistent_buffers\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2358\u001b[0m )\n\u001b[1;32m   2359\u001b[0m local_state \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_name_params \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "for i_episode in range(episode, num_episodes):\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    running_loss = 0.0\n",
        "    steps_done += 1\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "\n",
        "        observation, reward, termination, truncation, info = env.last()\n",
        "        done = termination or truncation\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            action = None  # Agent is dead\n",
        "            env.step(action)\n",
        "        else:\n",
        "            agent_handle = agent.split(\"_\")\n",
        "            agent_id = agent_handle[1]\n",
        "            agent_team = agent_handle[0]\n",
        "            if agent_team == \"blue\":\n",
        "                \n",
        "                buffer.update_last_reward(agent_id, reward) # update reward of last agent's action (bad environment!)\n",
        "                \n",
        "                action = policy(observation, policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "                try:\n",
        "                    next_observation = env.observe(agent)\n",
        "                    agent_done = False\n",
        "                except:\n",
        "                    next_observation = None\n",
        "                    agent_done = True\n",
        "\n",
        "                reward = 0 # Wait for next time to be selected to get reward\n",
        "                \n",
        "                # Store the transition in buffer\n",
        "                buffer.push(agent_id, observation, action, reward, next_observation, agent_done)\n",
        "\n",
        "                # Perform one step of the optimization (on the policy network)\n",
        "                optimize_model()\n",
        "\n",
        "                # Soft update of the target network's weights\n",
        "                # θ′ ← τ θ + (1 −τ )θ′\n",
        "                target_net_state_dict = target_net.state_dict()\n",
        "                policy_net_state_dict = policy_net.state_dict()\n",
        "                for key in policy_net_state_dict:\n",
        "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "                target_net.load_state_dict(target_net_state_dict)\n",
        "                \n",
        "            else:\n",
        "                # red agent\n",
        "                action = policy(observation, red_policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "        # Periodically update the red agent's policy with the blue agent's learned policy\n",
        "        if i_episode % 4 == 0 and i_episode < 24:\n",
        "            # Copy all weights and biases from the blue agent's policy network to the red agent's\n",
        "            red_policy_net.load_state_dict(policy_net.state_dict())\n",
        "        elif i_episode == 24: # more complex (pretrained) opponent\n",
        "            red_policy_net.load_state_dict(pretrained_net.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "    # Add these lines at the end of each episode\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_losses.append(running_loss)\n",
        "\n",
        "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
        "    print(f'Total Reward of previous episode: {episode_reward:.2f}')\n",
        "    print(f'Average Loss: {running_loss:.4f}')\n",
        "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
        "    print('-' * 40)\n",
        "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models/blue_{i_episode}.pt\")\n",
        "\n",
        "plot_metrics(episode_rewards, episode_losses, show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
