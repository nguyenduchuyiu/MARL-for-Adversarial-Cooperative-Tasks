{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB1Z5lTngHS5",
        "outputId": "d9dfa06d-a03e-4078-c650-c6d73df8f64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-wn48zx32\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-wn48zx32\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Collecting pettingzoo>=1.23.1 (from magent2==0.3.3)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: magent2\n",
            "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696076 sha256=5f08229e24ba941ec3c740be94424cd62d4c50d089ce5179815714cbbd194be3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kr4er03r/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\n",
            "Successfully built magent2\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo, magent2\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 magent2-0.3.3 pettingzoo-1.24.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDT2arkfhBrc"
      },
      "source": [
        "# QNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zWJY1eB1zT6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93525e11-34ac-40af-b9a2-96d6c85f439d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0414,  0.0612, -0.0647, -0.1421,  0.0311, -0.0413,  0.0350,  0.0335,\n",
              "          0.0120,  0.0380,  0.1036, -0.0682,  0.0391, -0.0420, -0.1298, -0.0986,\n",
              "          0.0551, -0.0406,  0.1169,  0.0486, -0.0643]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, action_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "            x = x.unsqueeze(0)\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "        x = torch.fliplr(x).permute(0,3,1,2) # flip left-right because blue agent observe identically with red agent\n",
        "        x = self.cnn(x)\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        return self.network(x)\n",
        "\n",
        "test = QNetwork((13,13,5), 21)\n",
        "test_obs = torch.rand((13,13,5))\n",
        "test(test_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxkz7aDhBrd"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FR2i1CH2hBre"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from magent2.environments import battle_v4\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XiQBiMNhBre"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XqI4jY7whBre"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(episode_rewards, episode_losses, show_result=False):\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Value')\n",
        "\n",
        "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
        "    losses_t = torch.tensor(episode_losses, dtype=torch.float)\n",
        "\n",
        "    plt.plot(rewards_t.numpy(), label='Reward')\n",
        "    plt.plot(losses_t.numpy(), label='Loss')\n",
        "\n",
        "    if len(rewards_t) >= 5:\n",
        "        rewards_means = rewards_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        rewards_means = torch.cat((torch.zeros(4), rewards_means))\n",
        "        plt.plot(rewards_means.numpy(), label='Reward (mean)')\n",
        "\n",
        "    if len(losses_t) >= 5:\n",
        "        losses_means = losses_t.unfold(0, 5, 1).mean(1).view(-1)\n",
        "        losses_means = torch.cat((torch.zeros(4), losses_means))\n",
        "        plt.plot(losses_means.numpy(), label='Loss (mean)')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSjCqKM6hBre"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fskc0dibgK9X"
      },
      "outputs": [],
      "source": [
        "class MultiAgentReplayBuffer:\n",
        "    def __init__(self, capacity, observation_shape, action_shape):\n",
        "        self.capacity = capacity\n",
        "        self.observation_shape = observation_shape\n",
        "        self.action_shape = action_shape\n",
        "\n",
        "        # Use a defaultdict to automatically create deques for new agents\n",
        "        self.buffers = defaultdict(lambda: {\n",
        "            'obs': deque(maxlen=capacity),\n",
        "            'action': deque(maxlen=capacity),\n",
        "            'reward': deque(maxlen=capacity),\n",
        "            'next_obs': deque(maxlen=capacity),\n",
        "            'done': deque(maxlen=capacity),\n",
        "        })\n",
        "\n",
        "    def push(self, agent_id, obs, action, reward, next_obs, done):\n",
        "        self.buffers[agent_id]['obs'].append(obs)\n",
        "        self.buffers[agent_id]['action'].append(action)\n",
        "        self.buffers[agent_id]['reward'].append(reward)\n",
        "        self.buffers[agent_id]['next_obs'].append(next_obs)\n",
        "        self.buffers[agent_id]['done'].append(done)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        all_agent_ids = list(self.buffers.keys())\n",
        "        if not all_agent_ids:\n",
        "            return None  # No agents in the buffer\n",
        "\n",
        "        # Check if we have enough data to sample\n",
        "        total_transitions = sum(len(self.buffers[agent_id]['obs']) for agent_id in all_agent_ids)\n",
        "        if total_transitions < batch_size:\n",
        "            return None\n",
        "\n",
        "        # Collect transitions from all agents into a single list\n",
        "        all_transitions = []\n",
        "        for agent_id in all_agent_ids:\n",
        "            agent_buffer = self.buffers[agent_id]\n",
        "            for i in range(len(agent_buffer['obs'])):\n",
        "                all_transitions.append({\n",
        "                    'obs': agent_buffer['obs'][i],\n",
        "                    'action': agent_buffer['action'][i],\n",
        "                    'reward': agent_buffer['reward'][i],\n",
        "                    'next_obs': agent_buffer['next_obs'][i],\n",
        "                    'done': agent_buffer['done'][i]\n",
        "                })\n",
        "\n",
        "        # Sample indices from the combined transitions\n",
        "        indices = np.random.choice(len(all_transitions), batch_size, replace=False)\n",
        "\n",
        "        # Extract the sampled transitions\n",
        "        obs_batch = np.array([all_transitions[i]['obs'] for i in indices])\n",
        "        action_batch = np.array([all_transitions[i]['action'] for i in indices])\n",
        "        reward_batch = np.array([all_transitions[i]['reward'] for i in indices])\n",
        "        next_obs_batch = np.array([all_transitions[i]['next_obs'] for i in indices])\n",
        "        done_batch = np.array([all_transitions[i]['done'] for i in indices])\n",
        "\n",
        "        return {\n",
        "            'obs': obs_batch,\n",
        "            'action': action_batch,\n",
        "            'reward': reward_batch,\n",
        "            'next_obs': next_obs_batch,\n",
        "            'done': done_batch\n",
        "        }\n",
        "\n",
        "    def update_last_reward(self, agent_id, new_reward):\n",
        "        if agent_id not in self.buffers:\n",
        "            return\n",
        "        self.buffers[agent_id]['reward'][-1] = new_reward\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(self.buffers[agent_id]['obs']) for agent_id in self.buffers)\n",
        "\n",
        "    def clear(self, agent_id=None):\n",
        "        if agent_id:\n",
        "            self.buffers[agent_id]['obs'].clear()\n",
        "            self.buffers[agent_id]['action'].clear()\n",
        "            self.buffers[agent_id]['reward'].clear()\n",
        "            self.buffers[agent_id]['next_obs'].clear()\n",
        "            self.buffers[agent_id]['done'].clear()\n",
        "        else:\n",
        "            for agent_id in self.buffers:\n",
        "                self.clear(agent_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCOblMIChBrf"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlIwCKyQhBrf",
        "outputId": "cf7ae0e4-d91b-40e1-996c-59c7e42c8701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "No model found!\n"
          ]
        }
      ],
      "source": [
        "env = battle_v4.env(map_size=45, minimap_mode=False, step_reward=0.01,\n",
        "                        dead_penalty=-2, attack_penalty=-0.1, attack_opponent_reward=2,\n",
        "                        max_cycles=300, extra_features=False, render_mode=\"rgb_array\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.9\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 50\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "observation_shape = env.observation_space(\"blue_0\").shape\n",
        "action_shape = env.action_space(\"blue_0\").n\n",
        "\n",
        "# Initialize networks\n",
        "policy_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "red_policy_net = QNetwork(observation_shape, action_shape).to(device) # for self-play\n",
        "target_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "# Load pretrained\n",
        "pretrained_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "pretrained_net.load_state_dict(torch.load(\"models/red.pt\", map_location=device, weights_only=True))\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(\"models/blue.pt\", map_location=device, weights_only=True)\n",
        "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
        "    red_policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    episode = checkpoint[\"episode\"]\n",
        "    print(f\"Start with episode: {episode}\")\n",
        "except Exception as e:\n",
        "    print(f\"No model found!\")\n",
        "    episode = 0\n",
        "\n",
        "buffer = MultiAgentReplayBuffer(10000, observation_shape, action_shape)\n",
        "steps_done = episode\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "running_loss = 0.0\n",
        "num_episodes = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFoqOhYshBrf"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YzUclygdhBrf"
      },
      "outputs": [],
      "source": [
        "def save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path):\n",
        "    torch.save({\n",
        "        'episode': i_episode,\n",
        "        'policy_net_state_dict': policy_net.state_dict(),\n",
        "        'target_net_state_dict': target_net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_losses': episode_losses,\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkGuIzzBhBrg"
      },
      "source": [
        "## Greedy Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VsRED0MRhBrg"
      },
      "outputs": [],
      "source": [
        "def linear_epsilon(steps_done):\n",
        "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
        "\n",
        "def policy(observation, q_network):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    if sample < linear_epsilon(steps_done):\n",
        "        return env.action_space(\"red_0\").sample()\n",
        "    else:\n",
        "        observation = (\n",
        "            torch.Tensor(observation).to(device)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            q_values = q_network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9tVYFvihBrg"
      },
      "source": [
        "# Optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A4XB3G5LhBrg"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    global running_loss\n",
        "\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # Handle cases where the buffer doesn't have enough samples yet\n",
        "    if batch is None:\n",
        "        return\n",
        "\n",
        "    # Unpack the batch\n",
        "    state_batch = torch.from_numpy(batch['obs']).float().to(device)\n",
        "    action_batch = torch.from_numpy(batch['action']).long().to(device)\n",
        "    reward_batch = torch.from_numpy(batch['reward']).float().to(device)\n",
        "    next_state_batch = torch.from_numpy(batch['next_obs']).float().to(device)\n",
        "    done_batch = torch.from_numpy(batch['done']).float().to(device)\n",
        "\n",
        "    # Reshape action_batch to (BATCH_SIZE, 1) for gather()\n",
        "    action_batch = action_batch.unsqueeze(1)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    non_final_mask = (done_batch == 0).squeeze()  # Create a mask for non-terminal states\n",
        "\n",
        "    # Only compute for non-terminal states\n",
        "    if non_final_mask.any():\n",
        "        next_state_values[non_final_mask] = target_net(next_state_batch[non_final_mask]).max(1).values.detach()\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEx209a7hBrg"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abzavvI2hBrg",
        "outputId": "16641d75-962a-4a89-9121-dcc370a8c975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/60\n",
            "Total Reward of previous episode: -306.10\n",
            "Average Loss: 15.0050\n",
            "Epsilon: 0.982\n",
            "----------------------------------------\n",
            "Episode 2/60\n",
            "Total Reward of previous episode: -389.30\n",
            "Average Loss: 29.2992\n",
            "Epsilon: 0.964\n",
            "----------------------------------------\n",
            "Episode 3/60\n",
            "Total Reward of previous episode: -256.00\n",
            "Average Loss: 43.3634\n",
            "Epsilon: 0.946\n",
            "----------------------------------------\n",
            "Episode 4/60\n",
            "Total Reward of previous episode: -334.86\n",
            "Average Loss: 54.0438\n",
            "Epsilon: 0.9279999999999999\n",
            "----------------------------------------\n",
            "Episode 5/60\n",
            "Total Reward of previous episode: -278.60\n",
            "Average Loss: 56.2159\n",
            "Epsilon: 0.91\n",
            "----------------------------------------\n",
            "Episode 6/60\n",
            "Total Reward of previous episode: -263.70\n",
            "Average Loss: 56.8774\n",
            "Epsilon: 0.892\n",
            "----------------------------------------\n",
            "Episode 7/60\n",
            "Total Reward of previous episode: -254.90\n",
            "Average Loss: 57.6357\n",
            "Epsilon: 0.874\n",
            "----------------------------------------\n",
            "Episode 8/60\n",
            "Total Reward of previous episode: -259.40\n",
            "Average Loss: 66.9795\n",
            "Epsilon: 0.856\n",
            "----------------------------------------\n",
            "Episode 9/60\n",
            "Total Reward of previous episode: -125.80\n",
            "Average Loss: 60.4524\n",
            "Epsilon: 0.838\n",
            "----------------------------------------\n",
            "Episode 10/60\n",
            "Total Reward of previous episode: -187.10\n",
            "Average Loss: 70.0521\n",
            "Epsilon: 0.82\n",
            "----------------------------------------\n",
            "Episode 11/60\n",
            "Total Reward of previous episode: -206.08\n",
            "Average Loss: 63.0142\n",
            "Epsilon: 0.802\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i_episode in range(episode, num_episodes):\n",
        "    env.reset()\n",
        "    episode_reward = 0\n",
        "    running_loss = 0.0\n",
        "    steps_done += 1\n",
        "\n",
        "    for agent in env.agent_iter():\n",
        "\n",
        "        observation, reward, termination, truncation, info = env.last()\n",
        "        done = termination or truncation\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            action = None  # Agent is dead\n",
        "            env.step(action)\n",
        "        else:\n",
        "            agent_handle = agent.split(\"_\")\n",
        "            agent_id = agent_handle[1]\n",
        "            agent_team = agent_handle[0]\n",
        "            if agent_team == \"blue\":\n",
        "\n",
        "                buffer.update_last_reward(agent_id, reward) # update reward of last agent's action (bad environment!)\n",
        "\n",
        "                action = policy(observation, policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "                try:\n",
        "                    next_observation = env.observe(agent)\n",
        "                    agent_done = False\n",
        "                except:\n",
        "                    next_observation = None\n",
        "                    agent_done = True\n",
        "\n",
        "                reward = 0 # Wait for next time to be selected to get reward\n",
        "\n",
        "                # Store the transition in buffer\n",
        "                buffer.push(agent_id, observation, action, reward, next_observation, agent_done)\n",
        "\n",
        "                # Perform one step of the optimization (on the policy network)\n",
        "                optimize_model()\n",
        "\n",
        "                # Soft update of the target network's weights\n",
        "                # θ′ ← τ θ + (1 −τ )θ′\n",
        "                target_net_state_dict = target_net.state_dict()\n",
        "                policy_net_state_dict = policy_net.state_dict()\n",
        "                for key in policy_net_state_dict:\n",
        "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "                target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "            else:\n",
        "                # red agent\n",
        "                action = policy(observation, red_policy_net)\n",
        "                env.step(action)\n",
        "\n",
        "        # Periodically update the red agent's policy with the blue agent's learned policy\n",
        "        if i_episode % 4 == 0 and i_episode < 24:\n",
        "            # Copy all weights and biases from the blue agent's policy network to the red agent's\n",
        "            red_policy_net.load_state_dict(policy_net.state_dict())\n",
        "        elif i_episode == 24: # more complex (pretrained) opponent\n",
        "            red_policy_net.load_state_dict(pretrained_net.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "    # Add these lines at the end of each episode\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_losses.append(running_loss)\n",
        "\n",
        "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
        "    print(f'Total Reward of previous episode: {episode_reward:.2f}')\n",
        "    print(f'Average Loss: {running_loss:.4f}')\n",
        "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
        "    print('-' * 40)\n",
        "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models/blue_{i_episode}.pt\")\n",
        "\n",
        "plot_metrics(episode_rewards, episode_losses, show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}